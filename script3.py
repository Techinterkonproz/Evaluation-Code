# This is the code to evaluate the RAG pipeline on the basis of RAGAS and GISKARD evaluation matrices 

#------------------------------------------------------------------------------------------------------
# Libraries to import for using evaluation 
import os
import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from llama_parse import LlamaParse
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from datasets import Dataset, load_dataset
from giskard.rag import KnowledgeBase, generate_testset, evaluate as giskard_evaluate
from ragas import evaluate as ragas_evaluate 
from ragas.metrics import (faithfulness, answer_relevancy, context_precision, context_recall)
from dotenv import load_dotenv
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib.colors import LinearSegmentedColormap

#------------------------------------------------------------------------------------------------------

# Loading the dot.env file that loads the api keys we need to use 
load_dotenv()

#------------------------------------------------------------------------------------------------------

# Creation of basic RAG pipeline for testing the evaluation metrics

# Parsing the PDF into the markdown

#------------------------------------------------------------------------------------

def parse_pdf_to_markdown(pdf_filepath: str, output_dir: str) -> str:
    if not os.path.exists(pdf_filepath):
        print(f"The file {pdf_filepath} does not exist. Please provide a valid path.")
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    markdown_output_path = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(pdf_filepath))[0]}.md")

    if os.path.exists(markdown_output_path):
        print(f"Markdown file already exists at: {markdown_output_path}. Using the existing file.")
    else:
        print("Parsing PDF and generating Markdown file...")
        parser = LlamaParse(result_type="markdown", num_workers=4, verbose=True, language="en")
        documents = parser.load_data(pdf_filepath)

        with open(markdown_output_path, 'w', encoding='utf-8') as file:
            for doc in documents:
                if doc.text.strip():
                    file.write(doc.text + "\n\n")
        print(f"Markdown file saved at: {markdown_output_path}")

    return markdown_output_path


#------------------------------------------------------------------------------------

# Creating chunks of the markdown of the given following FILE format

def split_markdown_to_chunks(md_content: str) -> list:
    headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)
    return markdown_splitter.split_text(md_content)

#------------------------------------------------------------------------------------

# Storing the created markdowns in the form of embeddings in the vector database

def create_vector_store(md_header_chunks: list) -> FAISS:
    embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
    vectorstore = FAISS.from_documents(md_header_chunks, embeddings)
    return vectorstore.as_retriever()

#------------------------------------------------------------------------------------

# Creating the basic RAG pipeline to query the question and response the result 
# In this case the query is generated by using the GISKARD prompt generation and using those query to generate respone from them  

def create_rag_chain(retriever) -> callable:
    template = """
    You are an assistant for question-answering tasks.
    Use the following pieces of retrieved context to answer the question.
    If you don't know the answer, just say that you don't know.
    Use ten sentences maximum and keep the answer as per the retrieved context.
    Question: {question}
    Context: {context}
    Answer:
    """
    prompt = ChatPromptTemplate.from_template(template)
    llm_model = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"), model_name="gpt-4o-mini")
    output_parser = StrOutputParser()
    
    return (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm_model
        | output_parser
    )

#------------------------------------------------------------------------------------

#------------------------------------------------------------------------------------------------------

# Here we are defining the knowledge base that uses the chunks generated for the embedding and storing them as a 
# Pandas Data Frame to be used as the knowledge base in the generation

def create_knowledge_base(md_header_chunks) -> KnowledgeBase:
    df = pd.DataFrame([doc.page_content for doc in md_header_chunks], columns=["text"])
    return KnowledgeBase(df)


#------------------------------------------------------------------------------------------------------

# In this we are using the Knowledge Base of Giskard to generate the Test Sets that stores the particular questions and 
# Their response on which the Giskard may evaluate from 

def generate_testset_from_knowledge_base(knowledge_base: KnowledgeBase) -> Dataset:
    testset = generate_testset(
        knowledge_base,
        num_questions=2,
        agent_description="A chatbot answering questions about the context available"
    )
    testset.save("testset.jsonl")
    return testset


#------------------------------------------------------------------------------------------------------

# In this function, we are calling the giskard evaluation to evaluate the pipeline using three parameters

def evaluate_giskard(answer_fn, testset, knowledge_base):
    report = giskard_evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)
    giskard_df = report.to_pandas()
    
    # Save Giskard scores in a CSV file for side-by-side comparison
    giskard_df['giskard_correctness'] = report.correctness_by_question_type().values
    giskard_df.to_csv("giskard_response.csv", index=False)

    # Generate Giskard evaluation heatmap
    correctness = report.correctness_by_question_type().values.reshape(-1, 1)
    plot_heatmap(correctness, "Giskard Correctness", "giskard_heatmap.pdf")


#------------------------------------------------------------------------------------------------------

# In this function, we evaluate the RAG pipeline based on the RAGAS evaluation metrics 

def evaluate_ragas(dataset):
    ragas_report = ragas_evaluate(dataset=dataset, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])
    ragas_df = ragas_report.to_pandas()
    
    # Add RAGAS metrics side-by-side with Giskard in the final CSV
    ragas_df[['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']].to_csv("ragas_response.csv", index=False)
    
    # Generate RAGAS evaluation heatmap
    metrics = ragas_df[['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']].values
    plot_heatmap(metrics, "RAGAS Metrics", "ragas_heatmap.pdf")


#------------------------------------------------------------------------------------------------------

# Plotting results of evaluation from both Giskard and RAGAS using matplotlib 

def plot_heatmap(data, title, output_path):
    os.makedirs("results", exist_ok=True)
    pdf_path = os.path.join("results", output_path)
    
    cmap = LinearSegmentedColormap.from_list("red_green", ["red", "green"])
    plt.figure(figsize=(10, 8))
    sns.heatmap(data, annot=True, fmt=".2f", cmap=cmap, linewidths=0.5)
    plt.title(title)
    
    with PdfPages(pdf_path) as pdf:
        pdf.savefig(bbox_inches='tight')
    plt.close()


#------------------------------------------------------------------------------------------------------

# Combining the Giskard and RAGAS results in a single CSV for side-by-side comparison

def save_combined_results():
    giskard_df = pd.read_csv("giskard_response.csv")
    ragas_df = pd.read_csv("ragas_response.csv")
    combined_df = pd.concat([giskard_df, ragas_df], axis=1)
    combined_df.to_csv("combined_evaluation_results.csv", index=False)
    print("Combined results saved as CSV at: combined_evaluation_results.csv")


#------------------------------------------------------------------------------------------------------

# Main function execution 

def main():
    
    pdf_filepath = input("Enter the path to the PDF file: ") # Providing the PDF file path to the system 
    
    local_dir = os.path.expanduser("~/Documents/pdf_to_md_files") # Loading the Markdown folder to store the markdown 
    
    markdown_output_path = parse_pdf_to_markdown(pdf_filepath, local_dir) # Calling the PDF-to-markdown function with file path and markdown folder path
    
    if markdown_output_path is None: # Checks if the markdown path is available or not 
        return

    with open(markdown_output_path, 'r', encoding='utf-8') as file:
        md_document_content = file.read() # Accessing the markdown file

    md_header_chunks = split_markdown_to_chunks(md_document_content) # Creating chunks from the generated markdown file  
    
    retriever = create_vector_store(md_header_chunks) # Creating the vector store to store embeddings in FAISS
    
    knowledge_base = create_knowledge_base(md_header_chunks) # Creating the Knowledge Base using the chunks for the embeddings
    
    testset = generate_testset_from_knowledge_base(knowledge_base) # Generating the test set 

    rag_chain = create_rag_chain(retriever) # Creating the RAG chain with the retriever
    
    test_df = testset.to_pandas() # Generating the Pandas DataFrame for the test set 
    
    test_questions = test_df["question"].values.tolist() # Getting list of questions from the generated DataFrame 
    
    test_groundtruths = test_df["reference_context"].values.tolist() # Getting list of reference contexts from the DataFrame 

    # Creating a dictionary to store data for RAGAS evaluation 
    data = {
        "question": test_questions,
        "answer": [rag_chain.invoke(query) for query in test_questions],
        "contexts": [[doc.page_content for doc in retriever.get_relevant_documents(query)] for query in test_questions],
        "ground_truth": test_groundtruths,
    }
    
    dataset = Dataset.from_dict(data) # Creating the dataset object from the dictionary 
    
    dataset.to_pandas().to_csv("response.csv", index=False) # Saving the dataset to CSV to show RAG pipeline responses
    
    print("Response saved as CSV at: response.csv")

    answer_fn = lambda question, history=None: rag_chain.invoke(question) # Setting up the RAG chain to invoke for answers
    
    evaluate_giskard(answer_fn, testset, knowledge_base) # Evaluating using Giskard 
    
    evaluate_ragas(dataset) # Evaluating using RAGAS

    # Save combined Giskard and RAGAS results side-by-side
    save_combined_results()
